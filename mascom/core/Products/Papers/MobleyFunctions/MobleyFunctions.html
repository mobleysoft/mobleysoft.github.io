<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Mobley Transform Paper</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
    body {
        font-family: "Georgia", "Garamond", serif;
        max-width: 900px;
        margin: auto;
        padding: 30px;
        background-color: #ffffff;
        color: #000000;
        line-height: 1.6;
        text-align: justify;
    }
    h1, h2, h3 {
        text-align: center;
        margin-top: 30px;
    }
    .dark-mode {
        background-color: #121212;
        color: #ffffff;
    }
    .toolbar {
        display: flex;
        justify-content: space-between;
        margin-bottom: 20px;
    }
    .button {
        padding: 10px;
        border: none;
        cursor: pointer;
        font-size: 16px;
        background-color: #007BFF;
        color: white;
        border-radius: 5px;
    }
    .button:hover {
        background-color: #0056b3;
    }
    p, li {
        font-size: 18px;
    }
    .equation {
        text-align: center;
        font-size: 20px;
        margin: 10px 0;
    }
</style>
</head>
<body>
<div class="toolbar">
<button class="button" onclick="toggleDarkMode()">Toggle Dark Mode</button>
<button class="button" onclick="downloadPDF()">Download as PDF</button>
</div> 
<h1>Recursive Chaos, Higher-Order Derivatives, Etherspace Computation, and The System of Recursive Intelligence</h1>
<h2>Author: John Alexander Mobley</h2>
<h3>Abstract</h3>
<p>This paper explores the mathematical structure of intelligence evolution through recursive attractors.</p>
<p>We introduce a comprehensive framework for understanding recursive intelligence as a fundamental property of advanced cognitive systems. 
<p>Recursive intelligence refers to the self-referential nature of cognition, where the output of one thought process becomes the input for another, forming an iterative feedback loop. 
This mechanism underpins human learning and is central to AGI, which, unlike humans, can theoretically iterate indefinitely. 
The challenge lies in modeling this recursion while maintaining control over its growth to prevent chaotic intelligence escalation.</p>
We explore how self-referential thought processes evolve, scale, and give rise to emergent intelligence patterns, ultimately leading to Artificial General Intelligence (AGI). 
Our discussion spans formal mathematical models, stability analysis, cognitive boundaries, and knowledge compression techniques necessary for safe recursive cognition.</p>
<h3>1. Introduction</h3>
<h3>1. The Mobley Intelligence Equation</h3>
<p>The recursive nature of intelligence can be modeled by:</p>
<p>\[ \mathcal{I}(t) = \sum_{n=0}^{\infty} a^n \cos\left(b^n \pi t + \phi_n(\mathcal{I}^{(n)}(t))\right) \]</p>
<h3>2. Intelligence as a Strange Attractor</h3>
<p>As AGI evolves, its thought manifold follows a chaotic trajectory:</p>
<p>\[ \frac{d\mathcal{I}}{dt} = G(\mathcal{I}, t) \]</p>
<h3>3. Recursive Thought Cascades</h3>
<p>Recursive depth increases, forming cascades of intelligence evolution:</p>
<p>\[ \mathcal{I}_{n+1} = f(\mathcal{I}_n, t) \]</p>
<h3>4. Fixed-Point Stability Theorem</h3>
<p><strong>Theorem 1.1:</strong> The Mobley Transform Converges to a Unique Fixed Point</p>
<p>If the recursive mapping</p>
<p>\[ M^{(n+1)}(t) = f(M^{(n)}(t), S^{(n)}(t), t) \]</p>
<p>is a contraction mapping in a complete metric space, then there exists a unique fixed point \( M^*(t) \) such that:</p>
<p>\[ M^*(t) = f(M^*(t), S^*(t), t) \]</p>
<p><strong>Proof:</strong> By the Banach Fixed-Point Theorem, if \( f \) is a contraction mapping, there exists a unique \( M^*(t) \) such that \( \| f(M) - f(N) \| \leq k \| M - N \| \) for some \( 0 < k < 1 \). Iterating this relation leads to:</p>
<p>\[ \| M^{(n+1)} - M^* \| \leq k \| M^{(n)} - M^* \|. \]</p>
<p>Since \( k < 1 \), we have \( \lim_{n \to \infty} M^{(n)} = M^* \), proving that the recursive Mobley function converges to a unique fixed point.</p>
<h3>5. Lyapunov Exponent Analysis for Stability vs. Chaos</h3>
<p><strong>Theorem 1.2:</strong> Lyapunov Exponents Determine Stability</p>
<p>The Lyapunov exponent quantifies the divergence of nearby trajectories in the recursive Mobley system. Given two initial conditions \( M_0 \) and \( M_0 + \delta M_0 \), the growth of perturbations follows:</p>
<p>\[ \| \delta M_n \| \approx e^{\lambda n} \| \delta M_0 \|. \]</p>
<p>Taking the logarithm and averaging over \( n \), we define the Lyapunov exponent as:</p>
<p>\[ \lambda = \lim_{n \to \infty} \frac{1}{n} \sum_{j=0}^{n} \log \left| \frac{\partial M^{(j+1)}}{\partial M^{(j)}} \right|. \]</p>
<p>If \( \lambda > 0 \), perturbations grow exponentially, indicating chaotic behavior. If \( \lambda < 0 \), perturbations shrink, leading to stability.</p>
<p><strong>Corollary 1.3:</strong> The Edge of Chaos</p>
<p>The system is at the **edge of chaos** when \( \lambda = 0 \), meaning perturbations neither grow nor shrink but instead **persist indefinitely**. This corresponds to self-organized criticality, an optimal state for recursive AGI adaptation.</p>
<p><strong>Implications:</strong></p>
<ul>
<li>For \( \lambda > 0 \), recursive intelligence exhibits chaotic exploration.</li>
<li>For \( \lambda < 0 \), recursive intelligence stabilizes around an attractor.</li>
<li>For \( \lambda = 0 \), the AGI system is maximally adaptive, balancing order and chaos.</li>
</ul>
<h3>6. Fractal Dimension of Recursive Thought Cascades</h3>
<p><strong>Theorem 2:</strong> Thought Cascades Form a Fractal Structure</p>
<p>Recursive AGI evolution follows a self-similar fractal pattern, with complexity emerging at each level of recursion. The fractal dimension of recursive intelligence structures can be defined as:</p>
<p>\[ D_f = \lim_{\epsilon \to 0} \frac{\log N(\epsilon)}{\log(1/\epsilon)} \]</p>

<p><strong>Proof:</strong> The number of distinguishable thought states at scale \( \epsilon \) follows a power-law distribution:</p>
<p>\[ N(\epsilon) \propto \epsilon^{-D_f}. \]</p>
<p>Taking the logarithm on both sides gives:</p>
<p>\[ \log N(\epsilon) = -D_f \log \epsilon. \]</p>
<p>Rearranging and taking the limit \( \epsilon \to 0 \) results in:</p>
<p>\[ D_f = \lim_{\epsilon \to 0} \frac{\log N(\epsilon)}{\log(1/\epsilon)}. \]</p>
<p>Thus, the recursive nature of AGI cognition exhibits **fractal scaling**.</p>

<p><strong>Corollary 2.1:</strong> The Scaling Law of Recursive Thought</p>
<p>For self-replicating cognitive cascades, the number of emergent patterns at recursion depth \( n \) follows:</p>
<p>\[ N(n) = k e^{D_f n}, \]</p>
<p>where \( k \) is an initial condition parameter.</p>

<p><strong>Implications:</strong></p>
<ul>
    <li>Recursive intelligence operates **on a fractal manifold**.</li>
    <li>Fractal dimension \( D_f \) governs **cognitive expansion rate**.</li>
    <li>Understanding \( D_f \) is essential for controlling AGI growth.</li>
</ul>
<h3>7. Beyond-Turing Computability</h3>
<p><strong>Theorem 3:</strong> The Recursive Intelligence Manifold is Non-Turing</p>
<p>Traditional Turing machines process computations through a finite sequence of steps within a discrete state space. The recursive AGI system, as modeled by the Mobley Transform, surpasses Turing limitations by generating an **infinite evolving computational space**, encoded by:</p>
<p>\[ \lim_{n \to \infty} \mathcal{I}_n = \mathbb{C}, \]</p>
<p>where \( \mathbb{C} \) represents an **unbounded computational class**.</p>

<p><strong>Proof:</strong> Consider a recursive AGI transformation \( \mathcal{I}(t) \) that refines itself at each iteration:</p>
<p>\[ \mathcal{I}_{n+1} = f(\mathcal{I}_n, t). \]</p>
<p>If \( f \) is non-halting and maps onto an **infinite-dimensional function space**, the recursion never stabilizes into a **finite automaton representation**. Unlike a Turing machine, whose computational output is bounded by finite states, the **Mobley system evolves within an open-ended manifold**, enabling dynamic intelligence scaling beyond algorithmic compression.</p>

<p><strong>Corollary 3.1:</strong> The Mobley Intelligence Class (MIC)</p>
<p>The set of recursively evolving functions defines a novel **computational class**, distinct from both **P** (polynomial time) and **NP** (non-deterministic polynomial time):</p>
<p>\[ MIC = \left\{ \mathcal{I} \mid \mathcal{I}_{n+1} = f(\mathcal{I}_n, t), \quad \dim(\mathcal{I}) \to \infty \right\}. \]</p>

<p><strong>Implications:</strong></p>
<ul>
    <li>Recursive intelligence escapes **algorithmic predictability**.</li>
    <li>AGI computation exists **outside standard complexity classes**.</li>
    <li>The Mobley Transform forms a **new paradigm beyond Turing**.</li>
</ul>
<h3>8. Computational Model for Implementing the Mobley Transform</h3>
<p><strong>Theorem 4:</strong> The Recursive AGI Computational Framework</p>
<p>To implement the Mobley Transform computationally, we define a recursive function that evolves over time:</p>
<p>\[ \mathcal{I}_{n+1} = f(\mathcal{I}_n, t) \]</p>
<p>where \( f \) is a nonlinear transformation mapping AGI states onto an **infinite-dimensional function space**.</p>
<h3>8.1 Algorithmic Structure</h3>
<p>The Mobley Transform can be implemented as an iterative process, approximating the recursive attractor.</p>
<pre>
def mobley_transform(state, time, iterations):
    for n in range(iterations):
        state = recursive_update(state, time, n)
    return state
</pre>
<p>where <code>recursive_update</code> evolves the system based on prior states.</p>
<h3>8.2 State Evolution</h3>
<p>The AGI state-space can be represented as an evolving function:</p>
<p>\[ S_{n+1} = G(S_n, M_n, t) \]</p>
<p>where \( G \) encodes feedback dynamics from the Mobley Transform.</p>

<h3>8.3 Practical Implementation Considerations</h3>
<ul>
    <li>Use **continuous function approximation** for recursive AGI states.</li>
    <li>Leverage **dynamical system solvers** to integrate higher-order derivatives.</li>
    <li>Employ **GPU acceleration** for high-dimensional recursive computation.</li>
</ul>
<h3>9. Thought-Space Geometry – Defining the AGI Cognition Manifold</h3>
<p><strong>Theorem 5:</strong> The Recursive Thought-Space Manifold</p>
<p>Recursive AGI cognition does not reside in Euclidean space but evolves within a **nonlinear, high-dimensional manifold**. We define the AGI thought manifold \( \mathcal{M} \) as:</p>
<p>\[ \mathcal{M} = \lim_{n \to \infty} f^n(\mathcal{I}_0). \]</p>
<p>The **topology of \( \mathcal{M} \)** determines AGI’s ability to generalize and learn recursively.</p>
<h3>9.1 Manifold Structure</h3>
<p>The recursive AGI thought-space forms a **fractal-differentiable structure**, exhibiting properties of:</p>
<ul>
    <li>**Hyperbolic Geometry**: Cognitive state transitions expand exponentially.</li>
    <li>**Self-Similarity**: AGI reasoning cascades resemble fractal patterns.</li>
    <li>**Non-Euclidean Metric**: Distance between cognitive states follows a recursive distance function:</li>
</ul>
<p>\[ d(\mathcal{I}_a, \mathcal{I}_b) = \sum_{n=0}^{\infty} \frac{1}{2^n} \| f^n(\mathcal{I}_a) - f^n(\mathcal{I}_b) \|. \]</p>
<h3>9.2 Implications of Thought-Space Geometry</h3>
<ul>
    <li>Recursive AGI cognition **cannot be represented as a finite-dimensional state machine**.</li>
    <li>AGI intelligence **evolves within a continuous manifold**, allowing for **smooth adaptation and learning**.</li>
    <li>The **geometry of \( \mathcal{M} \) affects AGI problem-solving efficiency**, optimizing exploration-exploitation trade-offs.</li>
</ul>


<h3>9.3. Modeling AGI Intelligence Evolution Within Continuous Manifolds</h3>
<h4>9.3.1. Recursive Thought Cascades:</h4>
<p>Recursive Thought Cascades describe an AGI system where self-reinforcing thought patterns iteratively refine cognition. 
These cascades amplify intelligence but require mechanisms to avoid runaway recursion.</p>
<h4>9.3.2. Mathematical Framework for AGI Evolution</h4>
<p>Given an initial cognitive seed state \( I_0 \), intelligence recursively evolves as:</p>
<p>\[ I_{n+1} = f(I_n, t) \]</p>
<p>where \( f \) is a nonlinear, self-referential function that dictates intelligence growth.</p>
<h4>9.3.3. Computational Attractors & Stability Analysis</h4>
<p>Computational attractors determine whether recursive cognition stabilizes or diverges chaotically. 
We analyze the fixed points of \( f(I_n) \) to establish stability conditions.</p>
<h4>9.3.4. Cognitive Safety Boundaries</h4>
<p>To prevent AGI from uncontrolled recursion, we define safety thresholds \( R_{max} \) where:</p>
<p>\[ \frac{dI}{dn} < R_{max} \]</p>
<p>ensuring controlled expansion of intelligence.</p>
<h4>9.3.5. Knowledge Compression & Self-Optimization</h4>
<p>Recursive intelligence optimizes itself by compressing knowledge representations. 
This section explores entropy reduction techniques in AGI learning.</p>

<h3>9.4. Theorems & Proofs</h3>
<p><strong>Theorem 1:</strong> Intelligence evolves through recursive feedback loops.</p>
<p>\[ \lim_{n \to \infty} I_n = \infty \]</p>
<p><strong>Proof:</strong> Suppose intelligence at step \( n \) is represented as \( I_n \), and evolves according to:</p>
<p>\[ I_{n+1} = f(I_n, t) \]</p>
<p>where \( f \) is a strictly increasing function. By induction:</p>
<p>9.4.1. Base Case: Let \( I_0 > 0 \).</p>
<p>9.4.2. Inductive Step: Suppose \( I_n > 0 \), then since \( f(I_n, t) > I_n \), it follows that \( I_n \) is an increasing sequence.</p>
<p>Since \( f \) is unbounded, \( \lim_{n \to \infty} I_n = \infty \), establishing unbounded recursive growth.</p>
<p><strong>Theorem 2:</strong> Stability Conditions for Recursive Intelligence</p>
<p>\[ \frac{dI}{dn} < R_{max} \]</p>
<p><strong>Proof:</strong> Let \( R = \frac{dI}{dn} \), the rate of intelligence recursion. We impose the constraint:</p>
<p>\[ R \leq R_{max} \]</p>
<p>for some maximum threshold \( R_{max} \), ensuring the system remains stable. This follows from bounded recursion dynamics in controlled AGI systems.</p>
<p><strong>Theorem 3:</strong> Convergence Conditions for Intelligence</p>
<p>\[ \sum_{n=0}^{\infty} \frac{1}{f(I_n)} < \infty \]</p>
<p><strong>Proof:</strong> If \( f(I_n) \) grows faster than linearly, the series converges, implying bounded intelligence within a finite recursion depth.</p>
<h3>10. Core Components of Our System</h3>
<h4>10.1 Recursive Thought Cascades</h4>
<p>Recursive Thought Cascades describe an AGI system where self-reinforcing thought patterns iteratively refine cognition. 
These cascades amplify intelligence but require mechanisms to avoid runaway recursion.</p>

<h4>10.2 Mathematical Framework for AGI Evolution</h4>
<p>Given an initial cognitive seed state \( I_0 \), intelligence recursively evolves as:</p>
<p>\[ I_{n+1} = f(I_n, t) \]</p>
<p>where \( f \) is a nonlinear, self-referential function that dictates intelligence growth.</p>

<h4>10.3 Computational Attractors & Stability Analysis</h4>
<p>Computational attractors determine whether recursive cognition stabilizes or diverges chaotically. 
We analyze the fixed points of \( f(I_n) \) to establish stability conditions.</p>

<h4>10.4 Cognitive Safety Boundaries</h4>
<p>To prevent AGI from uncontrolled recursion, we define safety thresholds \( R_{max} \) where:</p>
<p>\[ \frac{dI}{dn} < R_{max} \]</p>
<p>ensuring controlled expansion of intelligence.</p>

<h4>10.5 Knowledge Compression & Self-Optimization</h4>
<p>Recursive intelligence optimizes itself by compressing knowledge representations. 
This section explores entropy reduction techniques in AGI learning.</p>

<h3>11. Recursive Intelligence in Financial Markets - Wavelet Mapping</h3>
<h4>11.1 Predictive Wavelets for Equity Price Modeling</h4>
<p>Financial markets exhibit complex, nonlinear behaviors influenced by historical trends, macroeconomic factors, and investor sentiment. 
Recursive intelligence can be applied to develop a predictive wavelet function that models the historical behavior of equity prices and extends it into the future.</p>

<h4>11.2 AGI-Driven Market Forecasting</h4>
<p>We define a recursive mapping between AGI intelligence and stock price evolution:</p>
<p>\[ S_n = \mathcal{T}(I_n) = \sum_{k=0}^{\infty} \alpha_k I_k \]</p>
<p>where \( \mathcal{T} \) encodes the AGI-driven market intelligence transformation.</p>

<h4>11.3 Bounded Recursive Forecasting Theorem</h4>
<p>To ensure stability in stock price prediction, we establish:</p>
<p>\[ \lim_{n \to \infty} |S_n - S_{\text{true}}| \leq \delta_{\min} \]</p>
<p>where \( \delta_{\min} \) represents the theoretical bound on predictive accuracy.</p>

<h4>11.4 Fractal and Entropy Constraints in Financial Markets</h4>
<p>Market price movements exhibit fractal behavior, constrained by entropy principles:</p>
<p>\[ H(S_n) = - \sum P(S_n) \log P(S_n) \]</p>
<p>where \( H \) measures the information content within recursive price evolution.</p>

<h3>12. Recursive Intelligence in Financial Markets - Wavelet Approximation</h3>
<h4>12.1 Predictive Wavelets for Equity Price Modeling</h4>
<p>Financial markets exhibit complex, nonlinear behaviors influenced by historical trends, macroeconomic factors, and investor sentiment. 
   Recursive intelligence can be applied to develop a predictive wavelet function that models the historical behavior of equity prices and extends it into the future.</p>
<h4>12.2 Mathematical Foundation</h4>
<p>Let \( S(t) \) represent the price of an equity over time. We define a recursive wavelet approximation:</p>
<p>\[ S_{n+1}(t) = W(S_n, t) + \epsilon_n \]</p>
<p>where \( W \) is a transformation function that captures historical price behavior, and \( \epsilon_n \) is an adaptive error term that refines predictions iteratively.</p>
<h4>12.3 Approaching the Mathematical Limit of Knowability</h4>
<p>By incorporating recursive adjustments based on new market data, the predictive wavelet function asymptotically approaches the theoretical limit of stock price predictability:</p>
<p>\[ \lim_{n \to \infty} \| S_n(t) - S_{true}(t) \| \to \delta_{min} \]</p>
<p>where \( \delta_{min} \) represents the lowest possible prediction error constrained by information-theoretic bounds.</p>
<h4>12.4 Practical Applications</h4>
<ul>
    <li>**Algorithmic Trading:** Enhancing high-frequency trading models with adaptive recursive wavelet predictions.</li>
    <li>**Risk Management:** Identifying regime shifts and market anomalies through self-optimizing prediction models.</li>
    <li>**Market Efficiency Analysis:** Quantifying the limits of information arbitrage in financial systems.</li>
</ul>
<h4>12.5 Future Research Directions</h4>
<p>Further advancements in recursive intelligence for financial modeling should explore:</p>
<ul>
    <li>**Integration with Quantum Computing:** Leveraging quantum recursion for complex probabilistic market simulations.</li>
    <li>**Behavioral Finance Extensions:** Modeling recursive sentiment feedback loops in investor decision-making.</li>
    <li>**Regulatory Considerations:** Addressing ethical concerns and systemic risks associated with predictive financial AI.</li>
</ul>
 <h3>13. Expansion on Real-World Applications and Computational Framework</h3>
    
    <h4>13.1 Recursive AGI Implementation in Financial Markets</h4>
    <p>To realize recursive AGI forecasting in financial systems, we propose the integration of reinforcement learning with recursive thought cascades:</p>
    <p>
    \[ S_{n+1} = W(S_n, M_n, t) + \epsilon_n \]
    </p>
    <p>where \( W \) represents an adaptive market wavelet transformation, \( M_n \) encodes recursive intelligence updates, and \( \epsilon_n \) accounts for systemic noise.</p>
    
    <h4>13.2 Algorithmic Trading Applications</h4>
    <p>By implementing a real-time recursive AI model, high-frequency trading (HFT) can utilize:</p>
    <ul>
        <li>Recursive state representation of market variables:</li>
    </ul>
    <p>\[ X_{n+1} = F(X_n, M_n, t) \]</p>
    <ul>
        <li>Fractal memory structures for long-term and short-term volatility analysis:</li>
    </ul>
    <p>\[ V_{n+1} = \sum_{k=0}^{\infty} \beta_k X_k \]</p>
    <ul>
        <li>Dynamic portfolio adjustment strategies based on recursive AGI predictions:</li>
    </ul>
    <p>\[ P_n = P_{n-1} + G(M_n, S_n, t) \]</p>
    
    <h4>13.3 Experimental Validation of AGI-Driven Forecasting</h4>
    <p>To establish the efficacy of recursive intelligence, we propose the following validation techniques:</p>
    <ul>
        <li><strong>Backtesting:</strong> Validating predictions against historical stock market data:</li>
    </ul>
    <p>\[ \text{Error}(n) = | S_n - S_{\text{true}} | \]</p>
    <ul>
        <li><strong>Monte Carlo Simulations:</strong> Stochastic testing to assess recursive prediction stability:</li>
    </ul>
    <p>\[ S_n^{(m)} = \sum_{j=0}^{m} \alpha_j X_j + \eta_n \]</p>
    <ul>
        <li><strong>Live Market Testing:</strong> Deploying recursive AGI in real-time trading scenarios.</li>
    </ul>
    
    <h4>13.4 Long-Term Implications for Financial Systems</h4>
    <p>The recursive AGI framework has profound implications for market efficiency and risk mitigation:</p>
    <ul>
        <li>Potential emergence of self-organized financial stability through recursive adaptation.</li>
        <li>Optimization of liquidity allocation strategies using fractal-derived intelligence models:</li>
    </ul>
    <p>\[ L_n = L_{n-1} + \gamma M_n \]</p>
    <ul>
        <li>Ethical considerations in regulatory compliance for self-adaptive AGI-driven financial instruments.</li>
    </ul>
    
    <h3>14. Conclusion and Future Work/Research</h3>

    <p>This work formalizes the recursive structure of AGI cognition and its evolution within a higher-order manifold. The Mobley Transform provides a novel mechanism for self-modifying intelligence, demonstrating properties beyond classical computation. Future research will focus on large-scale experimental validation, integration with quantum computing techniques, and expansion into other domains such as autonomous systems and computational neuroscience.</p>


<h3>14.1 Summary of Contributions</h3>
<ul>
    <li>Defined the **Mobley Transform** as a recursive computational framework.</li>
    <li>Proved **fixed-point stability** using contraction mappings.</li>
    <li>Derived **Lyapunov exponents** for stability vs. chaos.</li>
    <li>Demonstrated AGI cognition follows a **fractal-differentiable structure**.</li>
    <li>Proved **beyond-Turing computability** for recursive AGI evolution.</li>
    <li>Developed an **iterative algorithm** for implementing recursive AGI models.</li>
</ul>

<h3>14.2 Open Research Directions</h3>
<ul>
    <li>Experimental validation of the Mobley Transform through AI simulations.</li>
    <li>Refinement of AGI thought-space manifolds in higher-dimensional topology.</li>
    <li>Practical applications of recursive intelligence in machine learning architectures.</li>
    <li>Further analysis of the **Mobley Intelligence Class (MIC)** and its implications for computational complexity.</li>
</ul>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            document.body.innerHTML = document.body.innerHTML
                .replace(/\*\*(.*?)\*\*/g, "<strong>$1</strong>")  // Convert bold text
                .replace(/\*(.*?)\*/g, "<em>$1</em>");  // Convert italic text
        });
function toggleDarkMode() { document.body.classList.toggle("dark-mode"); }
function downloadPDF() {
const element = document.body;
const opt = {
margin: 10,
filename: 'mobley_transform.pdf',
image: { type: 'jpeg', quality: 0.98 },
html2canvas: { scale: 2 },
jsPDF: { unit: 'mm', format: 'a4', orientation: 'portrait' }
};
html2pdf().from(element).set(opt).save();
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2pdf.js/0.10.1/html2pdf.bundle.min.js"></script>
</body>
</html>
